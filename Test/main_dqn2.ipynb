{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "#import gym\n",
    "#import gym_uav\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import count\n",
    "from collections import deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Custom Classes\n",
    "\n",
    "from Source.NN_model_2 import QNetwork\n",
    "from Source.dqn_agent_2 import Agent, ReplayBuffer, EpsilonGreedyStrategy\n",
    "from Source.Env_Manager import EnvManager\n",
    "from Source.Misc import plot\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper-parameters\n",
    "BUFFER_SIZE = int(1e5)      #replay buffer size\n",
    "BATCH_SIZE = 60             #minibatch size\n",
    "GAMMA = 0.999                #discount factor\n",
    "TAU = 1e-3                  #for soft update of target parameters\n",
    "LR = 5e-4                   #learning rate\n",
    "UPDATE_EVERY = 50            #how often to update the network\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.9985 #125e-6\n",
    "train_episodes = 3200\n",
    "test_episodes = 1\n",
    "seed = 0                    #random seed number\n",
    "episode_step_limit = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QNetwork(\n",
      "  (fc1): Linear(in_features=2, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (output): Linear(in_features=128, out_features=32, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Choose the environment\n",
    "em = EnvManager(device, 'uav-v2', seed)\n",
    "available_actions = em.num_actions_available()\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "#Select the strategy\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)\n",
    "\n",
    "#Initialize the agent\n",
    "agent = Agent(strategy, available_actions, seed, device)\n",
    "\n",
    "#Instantiate MemoryBuffer\n",
    "memory = ReplayBuffer(available_actions, BUFFER_SIZE, BATCH_SIZE, seed, device)\n",
    "\n",
    "policy_net = QNetwork(available_actions, seed).to(device)\n",
    "target_net = QNetwork(available_actions, seed).to(device)\n",
    "print(policy_net)\n",
    "\n",
    "#Initialize target_net weights to policy_net weights\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval() #Set the target_net in eval mode\n",
    "\n",
    "#Select the optimizer\n",
    "optimizer = optim.Adam(params=policy_net.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-fb281031b459>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx_ndx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mue_xloc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mue_xloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx_ndx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mue_yloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_ndx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhigh_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mexh_bdir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mexh_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_Exh_Rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0mrate_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_ndx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_ndx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexh_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecimals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Phd\\PyCharm_workspace\\Drone_RL\\gym_uav\\envs\\UAV_Env_v2.py\u001b[0m in \u001b[0;36mget_Exh_Rate\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mrbeam\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBeamSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#rbeam_vec:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mSNR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmimo_exh_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCalc_Rate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSF_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrbeam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[1;31m#rate = 1e3 * rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[0mexh_SNR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSNR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Phd\\PyCharm_workspace\\Drone_RL\\Source\\MIMO.py\u001b[0m in \u001b[0;36mCalc_Rate\u001b[1;34m(self, Tf, RB_ang)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;31m#energy_val = np.sqrt(self.N_tx*self.N_rx) * h * np.matmul(np.matmul(np.matmul(wRF.conj().T, a_rx), a_tx.conj().T),fRF)  # + np.matmul(wRF.conj().T, noise)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;31m#val = np.sqrt(self.N_tx * self.N_rx) * np.matmul(np.matmul(wRF.conj().T, h[:,:,i]), fRF)  # + np.matmul(wRF.conj().T, noise)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[0mrssi_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN_rx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mN_tx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwRF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfRF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m**\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;31m#+ np.conj(wRF.T).dot(noise))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;31m#energy_val += val\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mSNR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEs\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mrssi_val\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mN0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rate_tr = 0.0\n",
    "t_step = 0\n",
    "ue_xloc = em.env.ue_xloc  #10 locs\n",
    "ue_yloc = em.env.ue_yloc[::-1]     #5 locs\n",
    "\n",
    "rate_arr = np.zeros([(max(ue_yloc)-min(ue_yloc))//50 +1, (max(ue_xloc)-min(ue_xloc))//50 + 1]) #(-500,50) -----> (500,500)\n",
    "for y_ndx in range(len(ue_yloc)):\n",
    "    for x_ndx in range(len(ue_xloc)):\n",
    "        state = np.array([ue_xloc[x_ndx], ue_yloc[y_ndx]])/ em.env.high_obs\n",
    "        exh_bdir,exh_rate = em.env.get_Exh_Rate(state)\n",
    "        rate_arr[y_ndx,x_ndx] = np.around(exh_rate, decimals=3)\n",
    "\n",
    "row_labels = [str(x) for x in ue_yloc]\n",
    "col_labels = [str(x) for x in ue_xloc]\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "im = ax.imshow(rate_arr, aspect='auto')\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(rate_arr.shape[1]+1)-.5)\n",
    "ax.set_yticks(np.arange(rate_arr.shape[0]+1)-.5)\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(col_labels, fontsize=8)\n",
    "ax.set_yticklabels(row_labels, fontsize=8)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"center\",\n",
    "         rotation_mode=\"anchor\")\n",
    "plt.setp(ax.get_yticklabels(), rotation=0, ha=\"center\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(rate_arr.shape[0]):\n",
    "    for j in range(rate_arr.shape[1]):\n",
    "        text = ax.text(j, i, rate_arr[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\", fontsize=8)\n",
    "\n",
    "ax.set_title(\"Max possible Rates of UAV (w.r.t. BS)\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate_arr = rate_arr.flatten()\n",
    "\n",
    "plt.plot(rate_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ang=np.arange(np.pi/8, 10*np.pi/8, np.pi/8)\n",
    "print(ang)\n",
    "tan_ang=[]\n",
    "for a in ang:\n",
    "    tan_ang.append(np.arctan2(np.sin(a), np.cos(a)))\n",
    "    \n",
    "print(tan_ang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute and Set Rate Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rate_arr[100])\n",
    "rate_thr = rate_arr[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Environment with untrained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = em.reset(rate_thr)\n",
    "#state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "print(state.shape)\n",
    "for j in range(5):\n",
    "    action = agent.act(state, policy_net)\n",
    "    print(\"action: \", action.item())\n",
    "    #print(\"Action: {}, Qval: {}\".format(action, qval))\n",
    "    #env.render()\n",
    "    #print(\"[NB] action: {}\".format(action))\n",
    "    state, reward, done, _ = em.step(action)\n",
    "    #state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "em.env.render()      \n",
    "em.close()\n",
    "print(\"UAV Source: {0}, UAV dest: {1}\".format((em.env.ue_xsrc, em.env.ue_ysrc), (em.env.ue_xdest[0], em.env.ue_ydest[0]) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "nrows=len(ue_yloc)+1\n",
    "ncols=len(ue_xloc)+1\n",
    "\n",
    "\n",
    "xsrc_ndx = em.env.ue_xsrc\n",
    "\n",
    "map_str=\"\"\n",
    "row_str = \"\"\n",
    "for c_ndx in range(1, ncols):\n",
    "    if c_ndx == ncols:\n",
    "        row_str +=\"--\\n\"\n",
    "    else:\n",
    "        row_str += \"--\" \n",
    "map_str += row_str +\"\\n\"\n",
    "    \n",
    "for r_ndx in range(1,nrows):\n",
    "    row_str = \"\"\n",
    "    for c_ndx in range(1, ncols):\n",
    "        if c_ndx == ncols:\n",
    "            row_str +=\"|\\n\"\n",
    "        else:\n",
    "            row_str += \"|  \" #+ \"(\" + str(r_ndx) + \",\" + str(c_ndx) + \")\" \n",
    "    map_str += row_str +\"\\n\"\n",
    "    \n",
    "    row_str = \"\"\n",
    "    for c_ndx in range(1, ncols):\n",
    "        if c_ndx == ncols:\n",
    "            row_str +=\"--\\n\"\n",
    "        else:\n",
    "            row_str += \"--\" \n",
    "    map_str += row_str +\"\\n\"\n",
    "    \n",
    "print(map_str)\n",
    "'''    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "episode_durations = []\n",
    "episode_rewards = []\n",
    "policy_net.train()\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "    state = em.reset(rate_thr)\n",
    "    \n",
    "    #state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    ep_loss = 0.0\n",
    "    ep_rwd = 0.0\n",
    "    timestep = 0\n",
    "    agent.current_step +=1\n",
    "    for timestep in count():\n",
    "        action = agent.act(state, policy_net)\n",
    "        next_state, reward, done, _ = em.step(action)\n",
    "        ep_rwd += reward.item()\n",
    "        memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        state = next_state\n",
    "        #state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        \n",
    "        if memory.can_provide_sample():\n",
    "            experiences = memory.sample()\n",
    "            states, actions, rewards, next_states, dones = experiences\n",
    "            \n",
    "            #print(states.shape, states.dtype)\n",
    "            #print(actions.unsqueeze(-1).shape)\n",
    "            current_q_values = policy_net(states).gather(1,index=actions.unsqueeze(-1))\n",
    "            next_q_values = target_net(next_states).detach().max(1)[0]\n",
    "            target_q_values = (next_q_values*GAMMA) + rewards\n",
    "            \n",
    "            loss = F.mse_loss(current_q_values, target_q_values.unsqueeze(1))\n",
    "            #print(\"loss: \", loss.item())\n",
    "            ep_loss += loss.item()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if (timestep==episode_step_limit) or done:\n",
    "            episode_durations.append(timestep)\n",
    "            episode_rewards.append(ep_rwd)\n",
    "            print('\\rEpisode {}\\t timestep: {},\\tScore: {:.2f}, eps: {}'.format(episode, timestep, ep_rwd, agent.strategy.get_exploration_rate(agent.current_step)))\n",
    "            plot(episode_rewards, 100)\n",
    "            break\n",
    "            \n",
    "        \n",
    "    if episode % UPDATE_EVERY == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "    if (np.mean(episode_rewards[-100:]) >= 20000):\n",
    "        print(\"Goal is reached in {} episodes!\\n\".format(episode))\n",
    "        break\n",
    "        \n",
    "    torch.save(policy_net.state_dict(), 'checkpoint.pth')\n",
    "    #em.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing the DQN agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights from file\n",
    "policy_net.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "rate_acc_los_scores = []\n",
    "rate_acc_exh_scores = []\n",
    "learnt_val_fns = []\n",
    "true_val_fns = []\n",
    "test_scores = []\n",
    "policy_net.eval()\n",
    "for i_episode in range(1):\n",
    "    \n",
    "    learnt_rates = 0.0\n",
    "    los_rates = 0.0\n",
    "    exh_rates = 0.0\n",
    "    state_tensor = em.reset(rate_thr)\n",
    "    learnt_val_fn = 0.0\n",
    "    true_val_fn = 0.0\n",
    "    perf_score = 0\n",
    "    for t_step in count():\n",
    "        action = policy_net(state_tensor).argmax(dim=1).to(device)#agent.act(state_tensor, policy_net)\n",
    "        next_state_tensor, reward, done, _ = em.step(action)\n",
    "        \n",
    "        \n",
    "        state = np.rint(state_tensor.squeeze().cpu().data.numpy() * em.env.high_obs)\n",
    "        next_state = np.rint(next_state_tensor.squeeze().cpu().data.numpy() * em.env.high_obs)\n",
    "        print(\"Current State: \", state)\n",
    "        print(\"Next State: \", next_state)\n",
    "        \n",
    "        curr_loc = (state[0], state[1])\n",
    "        next_loc = (next_state[0], next_state[1])\n",
    "        #print(\"Test Episode: {2}, Current Location: {0}, Next Location: {1}\".format(curr_loc, next_loc, i_episode))\n",
    "        #print(\"Learnt Action: \", test_net(state))\n",
    "        \n",
    "            \n",
    "        next_state = next_state / em.env.high_obs\n",
    "        state = state / em.env.high_obs\n",
    "        \n",
    "        state_tensor = torch.tensor(np.array([next_state]), dtype=torch.float32).to(device)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #rate measurements\n",
    "        learnt_rate = em.env.get_Rate()\n",
    "        _,los_rate = em.env.get_Los_Rate(state)\n",
    "        exh_bdir,exh_rate = em.env.get_Exh_Rate(state)\n",
    "        \n",
    "        if (learnt_rate > em.env.rate_threshold):\n",
    "            perf_score +=1\n",
    "        learnt_val_fn += ((GAMMA**(t_step)) * (learnt_rate))\n",
    "        true_val_fn += ((GAMMA**(t_step)) * (exh_rate))\n",
    "        \n",
    "        rbd_ndx, ue_mv_ndx = em.env.decode_action(action)\n",
    "        ue_vx, ue_vy = em.env.choose_vel(ue_mv_ndx)\n",
    "        print(\"time step: \", t_step+1)\n",
    "        print(\"action parameters: ue_vx: {}, ue_vy: {}\".format(ue_vx, ue_vy))\n",
    "        print(\"exh dir: {0}, learnt bdir: {1}, learnt (AoA-AoD): {2}\".format(exh_bdir, em.env.BeamSet[rbd_ndx], em.env.aoa-em.env.aod))\n",
    "        print(\"Rwd: {3}, Learnt Rate: {0}, Exh_Rate: {1}, Los_Rate: {2}\\n\\n\".format(learnt_rate, exh_rate, los_rate, reward))\n",
    "        \n",
    "       \n",
    "        if (t_step== episode_step_limit) or done:\n",
    "            em.env.render()\n",
    "            break\n",
    "        '''\n",
    "        \n",
    "        #Move to next_state\n",
    "        state = next_state\n",
    "        \n",
    "        learnt_rates += learnt_rate\n",
    "        los_rates += los_rate\n",
    "        exh_rates += exh_rate\n",
    "        \n",
    "        if done:\n",
    "            rate_acc_los = learnt_rates/ los_rates\n",
    "            rate_acc_exh = learnt_rates / exh_rates\n",
    "            rate_acc_los_scores.append(rate_acc_los)\n",
    "            rate_acc_exh_scores.append(rate_acc_exh)\n",
    "            learnt_val_fns.append(learnt_val_fn)\n",
    "            true_val_fns.append(true_val_fn)\n",
    "            test_scores.append(perf_score)\n",
    "            #writer.add_scalar(\"test/rate_acc_los\", rate_acc_los, i_episode)\n",
    "            #writer.add_scalar(\"test/rate_acc_exh\", rate_acc_exh, i_episode)\n",
    "            print('\\rEpisode {}\\tRate_acc_los Score: {:.2f}\\tRate_acc_exh Score: {:.2f}\\t perf_score: {:.2f}'.format(i_episode, rate_acc_los, rate_acc_exh, perf_score, end=\"\"))\n",
    "            print(\"\\n\\n\")\n",
    "            break\n",
    "        '''\n",
    "#print(\"Total Epsiodes {},Average Rate_Acc_los: {:.2f}, Average Rate_Acc_exh: {:.2f}\\n\".format(test_episodes, np.mean(rate_acc_los_scores), np.mean(rate_acc_exh_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the weights from file\n",
    "policy_net.load_state_dict(torch.load('checkpoint.pth'))\n",
    "test_episodes = 20\n",
    "\n",
    "rate_acc_los_scores = []\n",
    "rate_acc_exh_scores = []\n",
    "perf_scores = []\n",
    "learnt_val_fns = []\n",
    "true_val_fns = []\n",
    "test_scores = []\n",
    "policy_net.eval()\n",
    "for i_episode in range(1, test_episodes+1):\n",
    "    \n",
    "    learnt_rates = 0.0\n",
    "    los_rates = 0.0\n",
    "    exh_rates = 0.0\n",
    "    state_tensor = em.reset(rate_thr)\n",
    "    learnt_val_fn = 0.0\n",
    "    true_val_fn = 0.0\n",
    "    perf_score = 0\n",
    "    \n",
    "    for t_step in count():\n",
    "        action = policy_net(state_tensor).argmax(dim=1).to(device)#agent.act(state_tensor, policy_net)\n",
    "        \n",
    "        next_state_tensor, reward, done, _ = em.step(action)\n",
    "        \n",
    "        state = np.rint(state_tensor.squeeze().cpu().data.numpy() * em.env.high_obs)\n",
    "        next_state = np.rint(next_state_tensor.squeeze().cpu().data.numpy() * em.env.high_obs)\n",
    "        #print(\"Current State: \", state)\n",
    "        #print(\"Next State: \", next_state)\n",
    "\n",
    "        curr_loc = (state[0], state[1])\n",
    "        next_loc = (next_state[0], next_state[1])\n",
    "        #print(\"Test Episode: {2}, Current Location: {0}, Next Location: {1}\".format(curr_loc, next_loc, i_episode))\n",
    "        #print(\"Learnt Action: \", test_net(state))\n",
    "        \n",
    "            \n",
    "        next_state = next_state / em.env.high_obs\n",
    "        \n",
    "        state_tensor = torch.tensor(np.array([next_state]), dtype=torch.float32).to(device)\n",
    "        \n",
    "        #rate measurements\n",
    "        learnt_rate = em.env.get_Rate()\n",
    "        _,los_rate = em.env.get_Los_Rate(next_state)\n",
    "        exh_bdir,exh_rate = em.env.get_Exh_Rate(next_state)\n",
    "        \n",
    "        learnt_val_fn += ((GAMMA**(t_step)) * (learnt_rate))\n",
    "        true_val_fn += ((GAMMA**(t_step)) * (exh_rate))\n",
    "        \n",
    "        #rbd_ndx, ue_vx_ndx, ue_vy_ndx = em.env.decode_action(action)\n",
    "        #print(\"time step: \", t_step+1)\n",
    "        #print(\"action parameters: ue_vx: {0}, ue_vy: {1}\".format(em.env.ue_vx[ue_vx_ndx], em.env.ue_vx[ue_vx_ndx]))\n",
    "        #print(\"exh dir: {0}, learnt bdir: {1}\".format(exh_bdir, em.env.BeamSet[rbd_ndx]))\n",
    "        #print(\"Rwd: {3}, Learnt Rate: {0}, Exh_Rate: {1}, Los_Rate: {2}\\n\\n\".format(learnt_rate, exh_rate, los_rate, reward))\n",
    "    \n",
    "        if (learnt_rate >= em.env.rate_threshold):\n",
    "            perf_score +=1\n",
    "\n",
    "        los_rates+=los_rate\n",
    "        exh_rates+=exh_rate\n",
    "        learnt_rates+=learnt_rate\n",
    "        \n",
    "        if (t_step== episode_step_limit) or done:\n",
    "            rate_acc_los = learnt_rates/ los_rates\n",
    "            rate_acc_exh = learnt_rates / exh_rates\n",
    "            print(\"\\rTest epsiode: {}\\t rate_acc_los: {:.2f}\\trate_acc_exh: {:.2f}, episode_len: {}, perf_score: {}\".format(i_episode, rate_acc_los, rate_acc_exh, t_step, perf_score))\n",
    "            rate_acc_los_scores.append(rate_acc_los)\n",
    "            rate_acc_exh_scores.append(rate_acc_exh)\n",
    "            #perf_scores.append(perf_score / t_step)\n",
    "            learnt_val_fns.append(learnt_val_fn)\n",
    "            true_val_fns.append(true_val_fn)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(learnt_val_fns)\n",
    "#print(true_val_fns)\n",
    "print(\"Avg rate_los_acc: {}, Avg rate_exh_acc: {}\".format(np.mean(rate_acc_los_scores), np.mean(rate_acc_exh_scores)))\n",
    "\n",
    "\n",
    "\n",
    "# plot the histogram\n",
    "w=0.3\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "x_axis = np.arange(test_episodes)\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(x_axis-w, learnt_val_fns, w, color='b')\n",
    "rects2= ax.bar(x_axis, true_val_fns, w, color='g')\n",
    "#plt.legend(loc='upper right')\n",
    "ax.legend( (rects1[0], rects2[0]), ('rl_val_fn', 'exh_val_fn'), loc='upper right' )\n",
    "plt.ylabel('Value Function')\n",
    "plt.xlabel('Episode #')\n",
    "ax.set_xticks(x_axis)\n",
    "ax.set_yticks(np.arange(0.0,100.0,10))\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        #ax.text(rect.get_x()+rect.get_width()/2., 1.03*h, '%.2f'%np.around(h, decimals=2),\n",
    "        #        ha='center', va='bottom')\n",
    "\n",
    "ax.axhline(np.mean(learnt_val_fns), color='b', linewidth=0.75, linestyle='--')\n",
    "ax.axhline(np.mean(true_val_fns), color='g', linewidth=0.75, linestyle='--')\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
