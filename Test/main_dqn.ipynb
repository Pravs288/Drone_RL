{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "source": [
    "Deep Q Network for UAV-0 MIMO environment\n",
    "\n",
    "In this notebook, a DQN network is implemented for openAI gym's UAV environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorboardX'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-479d8f8fdd72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorboardX\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSummaryWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorboardX'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import gym_uav\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "log = gym.logger\n",
    "log.set_level(gym.logger.INFO)\n",
    "\n",
    "#Tensorboard Writer\n",
    "writer = SummaryWriter()\n",
    "\n",
    "env = gym.make('uav-v0')\n",
    "env.seed(0)\n",
    "state_size = env.obs_space.shape[0]\n",
    "action_size = env.act_space.n\n",
    "print('State shape: ', env.obs_space.shape)\n",
    "print('Number of actions: ', env.act_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from Source.dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "\n",
    "# watch an untrained agent\n",
    "state = env.reset()\n",
    "print(state)\n",
    "for j in range(5):\n",
    "    action, qval = agent.act(state)\n",
    "    #print(\"Action: {}, Qval: {}\".format(action, qval))\n",
    "    #env.render()\n",
    "    #print(\"[NB] action: {}\".format(action))\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Train the Agent with DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def dqn_train(n_episodes=8000, eps_start=1.000, eps_end=.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []      #list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100) #last 100 scores\n",
    "    eps = eps_start\n",
    "    max_t = 5\n",
    "    ep_qvals = []\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset() # normalized state\n",
    "        score = 0.0\n",
    "        ep_qval = []\n",
    "        ep_loss = []\n",
    "        for t in range(max_t):\n",
    "            action, qval = agent.act(state, eps)\n",
    "            \n",
    "            #print(\"Action: {}, Qval: {}\".format(action, qval))\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            #print(\"reward: \", reward)\n",
    "            loss = agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            ep_qval.append(qval)\n",
    "            if loss is not None:\n",
    "                ep_loss.append(loss)\n",
    "            if done:\n",
    "                break\n",
    "        scores_window.append(score)\n",
    "        scores.append(score)\n",
    "        ep_qvals.append(np.mean(ep_qval))\n",
    "    \n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        writer.add_scalar(\"train/avg_ep_qval\", np.mean(ep_qval), i_episode)\n",
    "        writer.add_scalar(\"train/epsilon\", eps, i_episode)\n",
    "        writer.add_scalar(\"train/ep_score\", score, i_episode)\n",
    "        writer.add_scalar(\"train/ep_loss\", np.sum(ep_loss), i_episode)\n",
    "        \n",
    "        #eps = eps_end + (eps_start-eps_end)*math.exp(-1*i_episode/eps_decay)\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if i_episode % n_episodes == 0: #np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn_train()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {}
   },
   "source": [
    "Testing the DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "test_episodes = 20\n",
    "max_t = 5\n",
    "gamma = 0.99\n",
    "\n",
    "rate_acc_los_scores = []\n",
    "rate_acc_exh_scores = []\n",
    "learnt_val_fns = []\n",
    "true_val_fns = []\n",
    "test_scores = []\n",
    "agent.qnetwork_local.eval()\n",
    "for i_episode in range(1, test_episodes+1):\n",
    "    \n",
    "    learnt_rates = 0.0\n",
    "    los_rates = 0.0\n",
    "    exh_rates = 0.0\n",
    "    state = env.reset()\n",
    "    learnt_val_fn = 0.0\n",
    "    true_val_fn = 0.0\n",
    "    perf_score = 0\n",
    "    for t in range(max_t):\n",
    "        action, qval = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        state = state * env.high_obs\n",
    "        next_state = next_state * env.high_obs\n",
    "        \n",
    "        curr_loc = (state[0]*np.cos(state[1]), state[0]*np.sin(state[1]))\n",
    "        next_loc = (next_state[0]*np.cos(next_state[1]), next_state[0]*np.sin(next_state[1]))\n",
    "        #print(\"Test Episode: {2}, Current Location: {0}, Next Location: {1}\".format(curr_loc, next_loc, i_episode))\n",
    "        #print(\"Learnt Action: \", test_net(state))\n",
    "        \n",
    "            \n",
    "        next_state = next_state / env.high_obs\n",
    "        \n",
    "        #rate measurements\n",
    "        learnt_rate = env.get_Rate()\n",
    "        _,los_rate = env.get_Los_Rate(next_state)\n",
    "        exh_bdir,exh_rate = env.get_Exh_Rate(next_state)\n",
    "        \n",
    "        if (learnt_rate > env.rate_threshold):\n",
    "            perf_score +=1\n",
    "        learnt_val_fn += ((gamma**t) * (learnt_rate))\n",
    "        true_val_fn += ((gamma**t) * (exh_rate))\n",
    "        \n",
    "        print(\"exh dir: {0}, learnt bdir: {1}\".format(exh_bdir, env.BeamSet[action]))\n",
    "        print(\"Rwd: {3}, Learnt Rate: {0}, Exh_Rate: {1}, Los_Rate: {2}\".format(learnt_rate, exh_rate, los_rate, reward))\n",
    "\n",
    "        \n",
    "        #Move to next_state\n",
    "        state = next_state\n",
    "        \n",
    "        learnt_rates += learnt_rate\n",
    "        los_rates += los_rate\n",
    "        exh_rates += exh_rate\n",
    "        \n",
    "        if done:\n",
    "            rate_acc_los = learnt_rates/ los_rates\n",
    "            rate_acc_exh = learnt_rates / exh_rates\n",
    "            rate_acc_los_scores.append(rate_acc_los)\n",
    "            rate_acc_exh_scores.append(rate_acc_exh)\n",
    "            learnt_val_fns.append(learnt_val_fn)\n",
    "            true_val_fns.append(true_val_fn)\n",
    "            test_scores.append(perf_score)\n",
    "            writer.add_scalar(\"test/rate_acc_los\", rate_acc_los, i_episode)\n",
    "            writer.add_scalar(\"test/rate_acc_exh\", rate_acc_exh, i_episode)\n",
    "            print('\\rEpisode {}\\tRate_acc_los Score: {:.2f}\\tRate_acc_exh Score: {:.2f}\\t perf_score: {:.2f}'.format(i_episode, rate_acc_los, rate_acc_exh, perf_score, end=\"\"))\n",
    "            print(\"\\n\\n\")\n",
    "            break\n",
    "\n",
    "print(\"Total Epsiodes {},Average Rate_Acc_los: {:.2f}, Average Rate_Acc_exh: {:.2f}\\n\".format(test_episodes, np.mean(rate_acc_los_scores), np.mean(rate_acc_exh_scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# plot the histogram\n",
    "w=0.3\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "x_axis = np.arange(test_episodes)\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(x_axis-w, learnt_val_fns, w, color='b')\n",
    "rects2= ax.bar(x_axis, true_val_fns, w, color='g')\n",
    "#plt.legend(loc='upper right')\n",
    "ax.legend( (rects1[0], rects2[0]), ('rl_val_fn', 'exh_val_fn'), loc='upper right' )\n",
    "plt.ylabel('Value Function')\n",
    "plt.xlabel('Episode #')\n",
    "ax.set_xticks(x_axis)\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        h = rect.get_height()\n",
    "        ax.text(rect.get_x()+rect.get_width()/2., 1.02*h, '%.2f'%np.around(h, decimals=2),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "ax.axhline(np.mean(learnt_val_fns), color='b', linewidth=0.75, linestyle='--')\n",
    "ax.axhline(np.mean(true_val_fns), color='g', linewidth=0.75, linestyle='--')\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "\n",
    "writer.add_figure(\"test/hist_val_fns\", fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,20))\n",
    "x_axis = np.arange(test_episodes)\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(x_axis, test_scores, w, color='b')\n",
    "\n",
    "#ax.legend( (rects1[0]), ('rl_rate>rate_threshold'), loc='upper right' )\n",
    "plt.ylabel('Test Episode Score')\n",
    "plt.xlabel('Episode #')\n",
    "ax.set_xticks(x_axis)\n",
    "ax.axhline(np.mean(test_scores), color='b', linewidth=0.75, linestyle='--')\n",
    "\n",
    "writer.add_figure(\"test/ep_perf_score\", fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "#Load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "test_episodes = 20\n",
    "max_t = 5\n",
    "gamma = 0.99\n",
    "\n",
    "\n",
    "agent.qnetwork_local.eval()\n",
    "\n",
    "#Plotting a random Test episode\n",
    "    \n",
    "learnt_rates = []\n",
    "los_rates = []\n",
    "exh_rates = []\n",
    "ue_loc = []\n",
    "state = env.reset()\n",
    "learnt_val_fn = 0.0\n",
    "true_val_fn = 0.0\n",
    "for t in range(max_t):\n",
    "    action, qval = agent.act(state)\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "    state = state * env.high_obs\n",
    "    next_state = next_state * env.high_obs\n",
    "\n",
    "    curr_loc = (state[0]*np.cos(state[1]), state[0]*np.sin(state[1]))\n",
    "    next_loc = (next_state[0]*np.cos(next_state[1]), next_state[0]*np.sin(next_state[1]))\n",
    "    #print(\"Test Episode: {2}, Current Location: {0}, Next Location: {1}\".format(curr_loc, next_loc, i_episode))\n",
    "    #print(\"Learnt Action: \", test_net(state))\n",
    "\n",
    "    next_state = next_state / env.high_obs\n",
    "\n",
    "    #rate measurements\n",
    "    learnt_rate = env.get_Rate()\n",
    "    _,los_rate = env.get_Los_Rate(next_state)\n",
    "    exh_bdir,exh_rate = env.get_Exh_Rate(next_state)\n",
    "\n",
    "    learnt_val_fn += ((gamma**t) * (learnt_rate))\n",
    "    true_val_fn += ((gamma**t) * (exh_rate))\n",
    "\n",
    "    print(\"exh dir: {0}, learnt bdir: {1}\".format(exh_bdir, env.BeamSet[action]))\n",
    "    print(\"Rwd: {3}, Learnt Rate: {0}, Exh_Rate: {1}, Los_Rate: {2}\".format(learnt_rate, exh_rate, los_rate, reward))\n",
    "    \n",
    "    learnt_rates.append(learnt_rate)\n",
    "    exh_rates.append(exh_rate)\n",
    "    los_rates.append(los_rate)\n",
    "    ue_loc.append(str([np.around(next_loc[0], decimals=3), np.around(next_loc[1], decimals=3)]))\n",
    "\n",
    "fig=plt.figure(figsize=(12,12))\n",
    "plt.plot(ue_loc, learnt_rates,'bx-', ue_loc, los_rates, 'r', ue_loc, exh_rates, 'g.-')\n",
    "plt.legend([\"rl_rate\", \"los_rate\", \"exh_rate\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "parameters = agent.qnetwork_local.parameters()\n",
    "print('layer1: ', agent.qnetwork_local.hidden_layers[0].weight)\n",
    "print('layer2: ', agent.qnetwork_local.hidden_layers[1].weight)\n",
    "#print('layer3: ', agent.qnetwork_local.hidden_layers[2].weight)\n",
    "print('output layer: ', agent.qnetwork_local.output.weight)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
